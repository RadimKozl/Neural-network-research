{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 287651293,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RadimKozl/Neural-network-research/blob/main/bayesian-graphsage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian GraphSAGE: Scalable protein function prediction on the ogbn-proteins dataset**\n",
        "Implementing multi-label classification with uncertainty estimation using JAX/NNX and PyArrow.\n",
        "\n",
        "## ***Dataset description: ogbn-proteins (Biological network)***\n",
        "\n",
        "On your laptop, you are working with a dataset from the **Open Graph Benchmark (OGB)** series, which represents the cutting edge in testing graph algorithms.\n",
        "\n",
        "- **Povaha dat:** Reprezentuje vztahy mezi proteiny v různých biologických organismech.\n",
        "- **Structure:**\n",
        "    - **Nodes (132,534):** Individual proteins.\n",
        "    - **Edges (39,473,625):** Represent 8 types of interactions (e.g. homology, co-expression).\n",
        "- **Input Features:** Nodes initially have no features of their own. Your process uses **8-dimensional edge vectors** (link features) that are aggregated into nodes as initial features.\n",
        "- **Target:** Multi-label classification. You predict **112 binary labels** at once, corresponding to the presence of a protein in various biological functions.\n",
        "\n",
        "## ***Teorie: Bayesian GraphSAGE***\n",
        "\n",
        "Your model in your laptop is not just a standard neural network, it's an inductive and probabilistic system.\n",
        "\n",
        "### ***A. GraphSAGE (Inductive learning)***\n",
        "\n",
        "Unlike transductive models (like GCN) that require the entire graph in memory, GraphSAGE learns the **aggregation function**. This means that after training, your model can process proteins it never saw during training.\n",
        "\n",
        "**Key mechanism (SAmple and aggreGatE):** The model selects a fixed number of neighbors for each node in (Neighbor Sampling) and performs a state update:\n",
        "\n",
        "$$\n",
        "h_v^{(k)} = \\sigma \\left( W^k \\cdot \\text{CONTACT} \\left( h_v^{(k-1)}, \\text{AGG} \\left( \\{ h_u^{(k-1)} , \\forall u \\in N(v) \\}  \\right) \\right) \\right)\n",
        "$$\n",
        "\n",
        "- You are using **Mean Aggregator** (neighbor averaging) in your laptop.\n",
        "\n",
        "### ***B. Bayesian Uncertainty (MC Dropout)***\n",
        "\n",
        "The reason the model is called \"Bayesian\" is the implementation of **Monte Carlo Dropout**.\n",
        "\n",
        "- **Theory:** Gal & Ghahramani proved that the application of Dropout during inference is mathematically equivalent to the approximation of Gaussian processes.\n",
        "- **Usage in a laptop:** Even when testing (inference), you leave Dropout on. If you run the model N times, you get a distribution of results. The mean is your prediction and the variance is your uncertainty.\n",
        "\n",
        "## ***Tutorial Description (Implementation process on a laptop)***\n",
        "\n",
        "The notebook is divided into logical blocks that form the production pipeline:\n",
        "\n",
        "### ***Phase 1: Data Layer (Hybrid Storage)***\n",
        "\n",
        "Here you implement the critical solution for large graphs:\n",
        "\n",
        "- **PyArrow (Parquet):** Use it to store 39 million edges and node properties. Thanks to memory-mapping, you only load what you need.\n",
        "- **SQLite:** Serves as a fast topological index. The model asks SQLite: \"Who are the neighbors of node 42?\" and SQLite returns the rows in the Parquet file.\n",
        "\n",
        "### ***Phase 2: Model Architecture in NNX***\n",
        "\n",
        "You are using **JAX NNX**, which is an object-oriented API.\n",
        "\n",
        "- **Neighbor Sampling:** At each training step, your dataloader randomly selects 10-25 neighbors for each layer. This prevents memory overflow (Memory Out of Memory).\n",
        "- **Multi-label Head:** The last layer has 112 outputs with **Sigmoid** activation.\n",
        "\n",
        "### ***Phase 3: Training and Optimization***\n",
        "\n",
        "- **Loss:** You use `Binary Cross Entropy` calculated over all 112 labels.\n",
        "- **JIT Kompilace:** Funkce `train_step` je obalena `@jax.jit`, což zkompiluje tvůj Python kód do vysoce optimalizovaného strojového kódu pro GPU/TPU (OpenXLA).\n",
        "\n",
        "## ***Why is this solution \"Production-Ready\"?***\n",
        "\n",
        "This approach, which you have in your laptop, solves the three biggest pain points of graphing tasks:\n",
        "\n",
        "1. **Memory:** Thanks to PyArrow and neighbor sampling, you don't need 128GB of RAM.\n",
        "2. **Speed:** JAX and XLA compilations allow you to train millions of edges in minutes.\n",
        "3. **Credibility:** The Bayesian element gives your predictions a \"certificate of certainty\", which is essential in biochemistry or industry.\n",
        "\n",
        "***Summary of components for your documentation:***\n",
        "\n",
        "|Element|Implementation in a laptop|Theoretical benefit|\n",
        "|--------|-------------------------|-------------------|\n",
        "|**Data**|PyArrow + SQLite|Efficient I/O for giant graphs|\n",
        "|**Model**|GraphSAGE (NNX)|Inductive capability (new nodes)|\n",
        "|**Inference**|MC Dropout|Uncertainty estimation (Bayesian)|\n",
        "|**Engine**|JAX / OpenXLA|Maximum HW acceleration|"
      ],
      "metadata": {
        "id": "ZqCmmtOd1Ivl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Environment settings***"
      ],
      "metadata": {
        "id": "KCR-vXC_1Ivn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/sample_data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-29T15:26:28.273098Z",
          "iopub.execute_input": "2025-12-29T15:26:28.273384Z",
          "iopub.status.idle": "2025-12-29T15:26:28.398531Z",
          "shell.execute_reply.started": "2025-12-29T15:26:28.273351Z",
          "shell.execute_reply": "2025-12-29T15:26:28.397657Z"
        },
        "id": "e3m8sn6f1Ivn"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove existing JAX installations\n",
        "!pip uninstall -y -qq jax jaxlib jax-cuda12-plugin"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-29T15:26:28.400302Z",
          "iopub.execute_input": "2025-12-29T15:26:28.400571Z",
          "iopub.status.idle": "2025-12-29T15:26:43.441604Z",
          "shell.execute_reply.started": "2025-12-29T15:26:28.400543Z",
          "shell.execute_reply": "2025-12-29T15:26:43.440711Z"
        },
        "id": "0TKrD7-01Ivo"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Install JAX\n",
        "!pip install -qq --upgrade \"jax[cuda12]\"\n",
        "!pip install tensorboard\n",
        "!pip install tensorboard-plugin-profile"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-29T15:30:53.846223Z",
          "iopub.execute_input": "2025-12-29T15:30:53.846972Z",
          "execution_failed": "2025-12-29T15:31:34.155Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq0lUjx91Ivo",
        "outputId": "9f2de176-fdb0-4222-f772-7c67900d1c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.4)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Collecting tensorboard-plugin-profile\n",
            "  Downloading tensorboard_plugin_profile-2.21.3-cp312-none-manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting xprof==2.21.3 (from tensorboard-plugin-profile)\n",
            "  Downloading xprof-2.21.3-cp312-none-manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting gviz_api>=1.9.0 (from xprof==2.21.3->tensorboard-plugin-profile)\n",
            "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from xprof==2.21.3->tensorboard-plugin-profile) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from xprof==2.21.3->tensorboard-plugin-profile) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from xprof==2.21.3->tensorboard-plugin-profile) (1.17.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.12/dist-packages (from xprof==2.21.3->tensorboard-plugin-profile) (3.1.4)\n",
            "Requirement already satisfied: etils>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from etils[epath]>=1.0.0->xprof==2.21.3->tensorboard-plugin-profile) (1.13.0)\n",
            "Collecting cheroot>=10.0.1 (from xprof==2.21.3->tensorboard-plugin-profile)\n",
            "  Downloading cheroot-11.1.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: fsspec>=2024.3.1 in /usr/local/lib/python3.12/dist-packages (from xprof==2.21.3->tensorboard-plugin-profile) (2025.3.0)\n",
            "Requirement already satisfied: gcsfs>=2024.3.1 in /usr/local/lib/python3.12/dist-packages (from xprof==2.21.3->tensorboard-plugin-profile) (2025.3.0)\n",
            "Requirement already satisfied: more-itertools>=2.6 in /usr/local/lib/python3.12/dist-packages (from cheroot>=10.0.1->xprof==2.21.3->tensorboard-plugin-profile) (10.8.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from cheroot>=10.0.1->xprof==2.21.3->tensorboard-plugin-profile) (4.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]>=1.0.0->xprof==2.21.3->tensorboard-plugin-profile) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[epath]>=1.0.0->xprof==2.21.3->tensorboard-plugin-profile) (4.15.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]>=1.0.0->xprof==2.21.3->tensorboard-plugin-profile) (3.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (3.13.2)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (from gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.32.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=0.11.15->xprof==2.21.3->tensorboard-plugin-profile) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.22.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.28.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.8.0)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.7.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (2025.11.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.72.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2024.3.1->xprof==2.21.3->tensorboard-plugin-profile) (3.3.1)\n",
            "Downloading tensorboard_plugin_profile-2.21.3-cp312-none-manylinux2014_x86_64.whl (5.8 kB)\n",
            "Downloading xprof-2.21.3-cp312-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cheroot-11.1.2-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.2/109.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: gviz_api, cheroot, xprof, tensorboard-plugin-profile\n",
            "Successfully installed cheroot-11.1.2 gviz_api-1.10.0 tensorboard-plugin-profile-2.21.3 xprof-2.21.3\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core dependencies\n",
        "%pip -qq install --upgrade jax jaxlib flax optax orbax-checkpoint grain\n",
        "%pip -qq install numpy matplotlib scipy\n",
        "%pip -qq install torch torchvision\n",
        "%pip -qq install datasets\n",
        "%pip -qq install msgpack requests tqdm\n",
        "%pip -qq install bitsandbytes\n",
        "%pip -qq install jraph\n",
        "%pip -qq install networkx\n",
        "%pip -qq install ogb\n",
        "%pip -qq install pyarrow\n",
        "%pip -qq install db-sqlite3\n",
        "%pip -qq install pandas polars\n",
        "%pip -qq install bitsandbytes numpyro langdetect\n",
        "%pip -qq install xprof\n",
        "%pip -qq install jax2onnx\n",
        "%pip -qq install onnx onnxruntime\n",
        "%pip -qq install kagglehub"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.449Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqIYdm831Ivo",
        "outputId": "bd5d8152-3f7c-435e-9dda-8705d3bc9041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/488.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.3/504.3 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for db-sqlite3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for db (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antiorm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.5/753.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.0/521.0 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m130.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.5/180.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Git LFS for large files\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.450Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvF3zqn_1Ivo",
        "outputId": "edaea9d3-9169-4491-b578-21fa44906165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "print(100*\"-\")\n",
        "%pip show jax\n",
        "print(100*\"-\")\n",
        "%pip show jaxlib\n",
        "print(100*\"-\")\n",
        "%pip show jax-cuda12-plugin\n",
        "print(100*\"-\")\n",
        "%pip show flax\n",
        "print(100*\"-\")\n",
        "%pip show optax\n",
        "print(100*\"-\")\n",
        "%pip show torch\n",
        "print(100*\"-\")\n",
        "%pip show torchvision\n",
        "print(100*\"-\")\n",
        "%pip show orbax-checkpoint\n",
        "print(100*\"-\")\n",
        "%pip show numpy\n",
        "print(100*\"-\")\n",
        "%pip show tqdm\n",
        "print(100*\"-\")\n",
        "%pip show datasets\n",
        "print(100*\"-\")\n",
        "%pip show msgpack\n",
        "print(100*\"-\")\n",
        "%pip show bitsandbytes\n",
        "print(100*\"-\")\n",
        "%pip show jraph\n",
        "print(100*\"-\")\n",
        "%pip show networkx\n",
        "print(100*\"-\")\n",
        "%pip show ogb\n",
        "print(100*\"-\")\n",
        "%pip show pyarrow\n",
        "print(100*\"-\")\n",
        "%pip show db-sqlite3\n",
        "print(100*\"-\")\n",
        "%pip show polars\n",
        "print(100*\"-\")\n",
        "%pip show pandas\n",
        "print(100*\"-\")\n",
        "%pip show grain\n",
        "print(100*\"-\")\n",
        "%pip show bitsandbytes\n",
        "print(100*\"-\")\n",
        "%pip show numpyro\n",
        "print(100*\"-\")\n",
        "%pip show langdetect\n",
        "print(100*\"-\")\n",
        "%pip show xprof\n",
        "print(100*\"-\")\n",
        "%pip show jax2onnx\n",
        "print(100*\"-\")\n",
        "%pip show onnx\n",
        "print(100*\"-\")\n",
        "%pip show onnxruntime\n",
        "print(100*\"-\")\n",
        "%pip show tensorboard-plugin-profile\n",
        "print(100*\"-\")\n",
        "%pip show numpy\n",
        "print(100*\"-\")\n",
        "%pip show matplotlib\n",
        "print(100*\"-\")\n",
        "%pip show scipy\n",
        "print(100*\"-\")\n",
        "%pip show kagglehub\n",
        "print(100*\"-\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.450Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd-mqEjM1Ivq",
        "outputId": "6629104e-72c3-43cb-a8af-66676d14721a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Name: jax\n",
            "Version: 0.8.2\n",
            "Summary: Differentiate, compile, and transform Numpy code.\n",
            "Home-page: https://github.com/jax-ml/jax\n",
            "Author: JAX team\n",
            "Author-email: jax-dev@google.com\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: jaxlib, ml_dtypes, numpy, opt_einsum, scipy\n",
            "Required-by: chex, dopamine_rl, equinox, flax, jax2onnx, jraph, numpyro, optax, orbax-checkpoint, orbax-export\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Name: jaxlib\n",
            "Version: 0.8.2\n",
            "Summary: XLA library for JAX\n",
            "Home-page: https://github.com/jax-ml/jax\n",
            "Author: JAX team\n",
            "Author-email: jax-dev@google.com\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: ml_dtypes, numpy, scipy\n",
            "Required-by: chex, dopamine_rl, jax, jraph, numpyro, optax, orbax-export\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Environment setup complete!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.450Z"
        },
        "id": "L7fVbWlG1Ivq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Import and configuration***"
      ],
      "metadata": {
        "id": "-b5VILzR1Ivq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import gc\n",
        "import kagglehub\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import pickle\n",
        "import zipfile\n",
        "import base64\n",
        "import shutil\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "from pathlib import Path\n",
        "import functools\n",
        "from functools import partial\n",
        "from typing import (\n",
        "    Any,\n",
        "    Tuple,\n",
        "    Callable,\n",
        "    Optional,\n",
        "    Sequence,\n",
        "    List,\n",
        "    Dict\n",
        ")\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "\n",
        "# JAX and Flax NNX\n",
        "import jax\n",
        "import jax.ops\n",
        "import jax.lax\n",
        "import jax.profiler\n",
        "import jax.numpy as jnp\n",
        "import jax.export as jax_export\n",
        "from jax import (\n",
        "    random,\n",
        "    jit,\n",
        "    value_and_grad,\n",
        "    remat\n",
        ")\n",
        "import jax.tree_util as tree_util\n",
        "import flax.nnx as nnx\n",
        "from flax.nnx import filterlib\n",
        "from flax.serialization import (\n",
        "    msgpack_serialize,\n",
        "    from_bytes\n",
        ")\n",
        "import orbax.checkpoint as ocp\n",
        "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n",
        "\n",
        "# Optimization\n",
        "import optax\n",
        "\n",
        "import jraph\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS, Predictive\n",
        "from numpyro.contrib.module import nnx_module\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# PyTorch for compatibility (GGUF conversion)\n",
        "import torch\n",
        "\n",
        "import jax2onnx\n",
        "from jax2onnx import onnx_function, to_onnx\n",
        "\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "\n",
        "from google.colab import userdata  # For HF token in Colab\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.450Z"
        },
        "id": "JT7xXhX31Ivq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "Uuj3OtPmAbup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Ensure JAX uses GPU if available***"
      ],
      "metadata": {
        "id": "EsR98YcT1Ivq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure JAX for GPU\n",
        "try:\n",
        "    jax.config.update('jax_platform_name', 'gpu')\n",
        "    print(\"JAX devices:\", jax.devices())\n",
        "except RuntimeError:\n",
        "    print(\"GPU not available, using CPU\")\n",
        "    jax.config.update('jax_platform_name', 'cpu')\n",
        "    print(\"JAX devices:\", jax.devices())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.450Z"
        },
        "id": "mxtTJIS81Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Setting up access to Hugging Face***"
      ],
      "metadata": {
        "id": "dKsbg39r1Ivr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_git_config(email, name):\n",
        "    try:\n",
        "        # Setting global user.email\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", email], check=True)\n",
        "        print(f\"Git user.email set to: {email}\")\n",
        "\n",
        "        # Setting the global user.name\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", name], check=True)\n",
        "        print(f\"Git user.name set to: {name}\")\n",
        "\n",
        "        # Check settings (optional)\n",
        "        email_output = subprocess.run([\"git\", \"config\", \"--global\", \"user.email\"], capture_output=True, text=True, check=True)\n",
        "        name_output = subprocess.run([\"git\", \"config\", \"--global\", \"user.name\"], capture_output=True, text=True, check=True)\n",
        "        print(f\"Check - Email: {email_output.stdout.strip()}\")\n",
        "        print(f\"Check - Name: {name_output.stdout.strip()}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error while setting up Git configuration: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-12-29T15:28:27.450Z"
        },
        "id": "Lhdjy__x1Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Load Kaggle dataset***"
      ],
      "metadata": {
        "id": "Khx_6HVr4IIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path_dataset = kagglehub.dataset_download(\"radimkzl/graph-version-of-proteins-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path_dataset)"
      ],
      "metadata": {
        "id": "i9EESdLW4QKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cae4e135"
      },
      "source": [
        "print(f\"Checking contents of: {path_dataset}\")\n",
        "!ls -F {path_dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ae4cd53"
      },
      "source": [
        "print(f\"Checking contents of: {path_dataset}/proteins_graph_dataset/\")\n",
        "!ls -F {path_dataset}/proteins_graph_dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SAGEConv layer***"
      ],
      "metadata": {
        "id": "X8PRBdrL1Ivr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGEConv(nnx.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, rngs=None):\n",
        "        self.linear = nnx.Linear(in_features * 2, out_features, rngs=rngs or nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x, senders, receivers):\n",
        "        mean_neighbor = jraph.segment_mean(x[senders], receivers, x.shape[0])\n",
        "        concatenated = jnp.concatenate([x, mean_neighbor], axis=-1)\n",
        "        return self.linear(concatenated)"
      ],
      "metadata": {
        "trusted": true,
        "id": "z6A4ZoPR1Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Bayesian GraphSAGE model***"
      ],
      "metadata": {
        "id": "AH9qU9eC1Ivr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianGraphSAGE(nnx.Module):\n",
        "    def __init__(self, in_features=8, hidden_features=256, out_features=112):\n",
        "        rngs = nnx.Rngs(0)\n",
        "        self.sage1 = SAGEConv(in_features, hidden_features, rngs)\n",
        "        self.sage2 = SAGEConv(hidden_features, out_features, rngs)\n",
        "        self.dropout = nnx.Dropout(0.2, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x, senders, receivers, training=False):\n",
        "        def core(x):\n",
        "            x = jax.nn.relu(self.sage1(x, senders, receivers))\n",
        "            x = self.sage2(x, senders, receivers)\n",
        "            return x\n",
        "\n",
        "        x = remat(core)(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return jax.nn.sigmoid(x)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tzaus_FQ1Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Loader for OGBN-Proteins***"
      ],
      "metadata": {
        "id": "Emmq3nVO1Ivr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OGBNProteinsLoader:\n",
        "    def __init__(self, edges_parquet, nodes_parquet):\n",
        "        self.edges = pq.read_table(edges_parquet).to_pandas()\n",
        "        self.nodes = pq.read_table(nodes_parquet).to_pandas()\n",
        "\n",
        "        self.features = np.stack(self.nodes[\"features\"].values).astype(np.float32)\n",
        "        self.labels   = np.stack(self.nodes[\"labels\"].values).astype(np.float32)\n",
        "\n",
        "        self.node_id_to_idx = {nid: i for i, nid in enumerate(self.nodes[\"node_id\"])}\n",
        "\n",
        "        self.train_idx = self.nodes[self.nodes[\"split\"] == \"train\"][\"node_id\"].values\n",
        "        self.val_idx   = self.nodes[self.nodes[\"split\"] == \"valid\"][\"node_id\"].values\n",
        "        self.test_idx  = self.nodes[self.nodes[\"split\"] == \"test\"][\"node_id\"].values\n",
        "\n",
        "        print(f\"Loaded: {len(self.nodes)} nodes, {len(self.edges)} edges\")\n",
        "\n",
        "    def get_neighbors(self, node_id, max_neighbors=25):\n",
        "        src = self.edges[self.edges[\"source\"] == node_id][\"target\"].values[:max_neighbors]\n",
        "        tgt = self.edges[self.edges[\"target\"] == node_id][\"source\"].values[:max_neighbors]\n",
        "        neigh = np.unique(np.concatenate([src, tgt]))\n",
        "        return neigh if len(neigh) > 0 else np.array([node_id])\n",
        "\n",
        "    def sample_batch(self, batch_size=64, max_neighbors=25, split=\"train\"):\n",
        "        idx = {\"train\": self.train_idx, \"valid\": self.val_idx, \"test\": self.test_idx}[split]\n",
        "        centers = np.random.choice(idx, batch_size, replace=False)\n",
        "        return [(c, self.get_neighbors(c, max_neighbors)) for c in centers]\n",
        "\n",
        "    def get_features(self, node_ids):\n",
        "        return self.features[[self.node_id_to_idx[nid] for nid in node_ids]]\n",
        "\n",
        "    def get_labels(self, node_ids):\n",
        "        return self.labels[[self.node_id_to_idx[nid] for nid in node_ids]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "cCaipbPX1Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Bayesian model and inference***"
      ],
      "metadata": {
        "id": "L75NWeOf1Ivr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_graphsage_model(graph_data, X, y=None):\n",
        "    senders, receivers, num_nodes_batch = graph_data\n",
        "\n",
        "    # Define priors for all model parameters with more reasonable scales\n",
        "    in_features, hidden_features, out_features = 8, 256, 112\n",
        "\n",
        "    # sage1 parameters - use wider priors for better exploration\n",
        "    w1_scale = numpyro.sample('w1_scale', dist.HalfNormal(1.0))\n",
        "    w1 = numpyro.sample('w1', dist.Normal(0, w1_scale).expand([in_features * 2, hidden_features]).to_event(2))\n",
        "    b1 = numpyro.sample('b1', dist.Normal(0, 0.5).expand([hidden_features]).to_event(1))\n",
        "\n",
        "    # sage2 parameters\n",
        "    w2_scale = numpyro.sample('w2_scale', dist.HalfNormal(1.0))\n",
        "    w2 = numpyro.sample('w2', dist.Normal(0, w2_scale).expand([hidden_features * 2, out_features]).to_event(2))\n",
        "    b2 = numpyro.sample('b2', dist.Normal(0, 0.5).expand([out_features]).to_event(1))\n",
        "\n",
        "    # Forward pass\n",
        "    # Layer 1\n",
        "    mean_neighbor = jraph.segment_mean(X[senders], receivers, num_nodes_batch)\n",
        "    concat1 = jnp.concatenate([X, mean_neighbor], axis=-1)\n",
        "    h = jax.nn.relu(concat1 @ w1 + b1)\n",
        "\n",
        "    # Layer 2\n",
        "    mean_neighbor2 = jraph.segment_mean(h[senders], receivers, num_nodes_batch)\n",
        "    concat2 = jnp.concatenate([h, mean_neighbor2], axis=-1)\n",
        "    logits = concat2 @ w2 + b2\n",
        "    preds = jax.nn.sigmoid(logits)\n",
        "\n",
        "    # Add small epsilon to avoid numerical issues\n",
        "    preds = jnp.clip(preds, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Likelihood\n",
        "    with numpyro.plate(\"data\", X.shape[0]):\n",
        "        numpyro.sample(\"obs\", dist.Bernoulli(probs=preds).to_event(1), obs=y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CAAVyhWE1Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mcmc(loader, batch_size=32, num_samples=500, warmup=500):\n",
        "    print(\"Preparing the training batch...\")\n",
        "    batch = loader.sample_batch(batch_size=batch_size, split=\"train\")\n",
        "\n",
        "    # Create node ID to local index mapping\n",
        "    all_nodes = set()\n",
        "    for center, neigh in batch:\n",
        "        all_nodes.add(center)\n",
        "        all_nodes.update(neigh)\n",
        "\n",
        "    all_nodes = sorted(list(all_nodes))\n",
        "    node_to_idx = {nid: i for i, nid in enumerate(all_nodes)}\n",
        "\n",
        "    # Build edges with local indices\n",
        "    senders, receivers = [], []\n",
        "    for center, neigh in batch:\n",
        "        center_idx = node_to_idx[center]\n",
        "        for n in neigh:\n",
        "            n_idx = node_to_idx[n]\n",
        "            senders += [center_idx, n_idx]\n",
        "            receivers += [n_idx, center_idx]\n",
        "\n",
        "    senders = jnp.array(senders)\n",
        "    receivers = jnp.array(receivers)\n",
        "    num_nodes_batch = len(all_nodes)\n",
        "\n",
        "    graph_data = (senders, receivers, num_nodes_batch)\n",
        "\n",
        "    # Get features and labels for all nodes in the batch\n",
        "    X = jnp.array(loader.get_features(all_nodes))\n",
        "    # Only get labels for center nodes\n",
        "    center_nodes = [c for c, _ in batch]\n",
        "    center_indices = jnp.array([node_to_idx[c] for c in center_nodes])\n",
        "\n",
        "    # Create full label array (only center nodes have labels)\n",
        "    y_full = jnp.zeros((num_nodes_batch, 112), dtype=jnp.float32)\n",
        "    y_centers = jnp.array(loader.get_labels(center_nodes))\n",
        "    y_full = y_full.at[center_indices].set(y_centers)\n",
        "\n",
        "    key = random.PRNGKey(42)\n",
        "\n",
        "    # Use more samples and longer warmup for better convergence\n",
        "    kernel = NUTS(bayesian_graphsage_model, max_tree_depth=6)\n",
        "    mcmc = MCMC(kernel, num_warmup=warmup, num_samples=num_samples, num_chains=1)\n",
        "\n",
        "    print(f\"Running MCMC (warmup={warmup}, samples={num_samples})...\")\n",
        "    mcmc.run(key, graph_data, X, y=y_full)\n",
        "\n",
        "    # Print diagnostics\n",
        "    mcmc.print_summary()\n",
        "\n",
        "    print(\"MCMC completed!\")\n",
        "\n",
        "    return mcmc.get_samples()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VMlcZYyi1Ivr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Prediction with uncertainty***"
      ],
      "metadata": {
        "id": "8vq7Zc2S1Ivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_uncertainty(posterior_samples, graph_data, X, n_samples=100):\n",
        "    senders, receivers, num_nodes_batch = graph_data\n",
        "\n",
        "    preds = []\n",
        "    for i in tqdm(range(n_samples), desc=\"MC prediction\"):\n",
        "        # Extract parameters for this sample\n",
        "        w1 = posterior_samples['w1'][i]\n",
        "        b1 = posterior_samples['b1'][i]\n",
        "        w2 = posterior_samples['w2'][i]\n",
        "        b2 = posterior_samples['b2'][i]\n",
        "\n",
        "        # Forward pass with sampled parameters\n",
        "        # Layer 1\n",
        "        mean_neighbor = jraph.segment_mean(X[senders], receivers, num_nodes_batch)\n",
        "        concat1 = jnp.concatenate([X, mean_neighbor], axis=-1)\n",
        "        h = jax.nn.relu(concat1 @ w1 + b1)\n",
        "\n",
        "        # Layer 2\n",
        "        mean_neighbor2 = jraph.segment_mean(h[senders], receivers, num_nodes_batch)\n",
        "        concat2 = jnp.concatenate([h, mean_neighbor2], axis=-1)\n",
        "        logits = concat2 @ w2 + b2\n",
        "        pred = jax.nn.sigmoid(logits)\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = jnp.stack(preds)\n",
        "    return preds.mean(0), preds.std(0)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ALMjZUP-1Ivs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Visualization of inference results***"
      ],
      "metadata": {
        "id": "yJIFtWfL1Ivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(mean_pred, uncertainty, true_labels, num_classes=30):\n",
        "    mean_node = mean_pred[0]\n",
        "    unc_node = uncertainty[0]\n",
        "    true_node = true_labels[0]\n",
        "\n",
        "    x = np.arange(num_classes)\n",
        "    classes = [f'C{i}' for i in range(num_classes)]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    ax.bar(x, mean_node[:num_classes], yerr=unc_node[:num_classes], capsize=5, color='skyblue', alpha=0.8, label='Pre-values ± uncertainty of values')\n",
        "    positive = np.where(true_node[:num_classes] > 0.5)[0]\n",
        "    ax.plot(positive, true_node[positive], 'ro', markersize=8, label='True positive')\n",
        "\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_title('Bayesian GraphSAGE – Prediction with uncertainty (first node)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(classes, rotation=45)\n",
        "    ax.legend()\n",
        "    ax.grid(True, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "LYOlQo0T1Ivs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Export do všech formátů***"
      ],
      "metadata": {
        "id": "ntCvNlEH1Ivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_model_all_formats(model, posterior_samples, output_dir=\"models\"):\n",
        "    \"\"\"\n",
        "    Fixed export with synchronous checkpointer to avoid async traceback\n",
        "    \"\"\"\n",
        "    import time\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"EXPORT MODELU (Synchronous PyTreeCheckpointer)\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Use synchronous checkpointer - no async threads\n",
        "    checkpointer = PyTreeCheckpointer()\n",
        "\n",
        "    # 1. Save model structure - separate graphdef and state\n",
        "    graphdef, param_state, rng_state = nnx.split(model, nnx.Param, nnx.RngState)\n",
        "\n",
        "    # Save graphdef separately using pickle (it's not a pytree)\n",
        "    model_dir = os.path.join(output_dir, \"bayesian_graphsage_nnx\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    graphdef_path = os.path.join(model_dir, \"graphdef.pkl\")\n",
        "    with open(graphdef_path, 'wb') as f:\n",
        "        pickle.dump(graphdef, f)\n",
        "    print(f\"✓ GraphDef uložen jako pickle: {graphdef_path}\")\n",
        "\n",
        "    # Save param_state and rng_state with Orbax (these ARE pytrees)\n",
        "    state_items = {\n",
        "        \"param_state\": param_state,\n",
        "        \"rng_state\": rng_state\n",
        "    }\n",
        "\n",
        "    state_path = os.path.join(model_dir, \"state\")\n",
        "    checkpointer.save(state_path, state_items)\n",
        "    print(f\"✓ Model state uložen do: {state_path}\")\n",
        "\n",
        "    # 2. Save posterior samples (stacked)\n",
        "    sample_keys = list(posterior_samples.keys())\n",
        "    num_samples = len(next(iter(posterior_samples.values())))\n",
        "\n",
        "    stacked_state = {}\n",
        "    for key in sample_keys:\n",
        "        stacked_state[key] = jnp.stack(posterior_samples[key])\n",
        "\n",
        "    posterior_state = nnx.State(stacked_state)\n",
        "\n",
        "    posterior_path = os.path.join(output_dir, \"posterior_samples\")\n",
        "    checkpointer.save(posterior_path, posterior_state)\n",
        "    print(f\"✓ Posterior samples uloženy ({num_samples} samples) do: {posterior_path}\")\n",
        "\n",
        "    # CRITICAL: Wait for Orbax to finalize (prevents async traceback)\n",
        "    print(\"\\nWaiting for the finalization of Orbax checkpoints...\")\n",
        "    time.sleep(2)  # Give Orbax time to finish async operations\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ALL DONE! Checkpoints are fully finalized.\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"   • bayesian_graphsage_nnx/\")\n",
        "    print(\"     ├── graphdef.pkl          — model structure\")\n",
        "    print(\"     └── state/                — model parameters\")\n",
        "    print(\"   • posterior_samples/        — MCMC samples\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BTfEPTea1Ivs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Memory-optimized inference - processes one sample at a time***"
      ],
      "metadata": {
        "id": "4GD89ESy1Ivs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_run_inference(loader, path_model_dir, n_mc=50, batch_size=16):\n",
        "    \"\"\"\n",
        "    Ultra memory-optimized inference - one sample at a time with immediate cleanup\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== LOADING THE MODEL AND POSTERIOR SAMPLES ===\\n\")\n",
        "\n",
        "    checkpointer = PyTreeCheckpointer()\n",
        "\n",
        "    # 1. Load model checkpoint (we don't actually need graphdef for inference)\n",
        "    model_dir = os.path.join(path_model_dir, \"bayesian_graphsage_nnx\")\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
        "\n",
        "    print(\"✓ Model directory found\")\n",
        "\n",
        "    # 2. Load posterior samples\n",
        "    posterior_path = os.path.join(path_model_dir, \"posterior_samples\")\n",
        "\n",
        "    if not os.path.exists(posterior_path):\n",
        "        candidates = glob.glob(os.path.join(path_model_dir, \"posterior_samples*\"))\n",
        "        if not candidates:\n",
        "            raise FileNotFoundError(\"No directory with posterior samples found!\")\n",
        "        posterior_path = sorted(candidates)[-1]\n",
        "\n",
        "    posterior_state = checkpointer.restore(posterior_path)\n",
        "    posterior_samples = {k: posterior_state[k] for k in posterior_state.keys()}\n",
        "    num_samples_total = posterior_state[list(posterior_state.keys())[0]].shape[0]\n",
        "\n",
        "    print(f\"✓ Loaded {num_samples_total} posterior samples\\n\")\n",
        "\n",
        "    # 3. Prepare test batch with SMALLER size to reduce memory\n",
        "    print(\"I'm preparing a test batch...\")\n",
        "    test_batch = loader.sample_batch(batch_size=batch_size, split=\"test\")\n",
        "\n",
        "    # Build node mapping for test batch\n",
        "    all_nodes = set()\n",
        "    for center, neigh in test_batch:\n",
        "        all_nodes.add(center)\n",
        "        all_nodes.update(neigh)\n",
        "\n",
        "    all_nodes = sorted(list(all_nodes))\n",
        "    node_to_idx = {nid: i for i, nid in enumerate(all_nodes)}\n",
        "\n",
        "    # Build edges with local indices\n",
        "    senders_list, receivers_list = [], []\n",
        "    for center, neigh in test_batch:\n",
        "        center_idx = node_to_idx[center]\n",
        "        for n in neigh:\n",
        "            n_idx = node_to_idx[n]\n",
        "            senders_list += [center_idx, n_idx]\n",
        "            receivers_list += [n_idx, center_idx]\n",
        "\n",
        "    senders = jnp.array(senders_list)\n",
        "    receivers = jnp.array(receivers_list)\n",
        "\n",
        "    # Get features and labels for all nodes\n",
        "    X_test = jnp.array(loader.get_features(all_nodes))\n",
        "\n",
        "    # Get labels only for center nodes\n",
        "    center_nodes = [c for c, _ in test_batch]\n",
        "    center_indices = jnp.array([node_to_idx[c] for c in center_nodes])\n",
        "    y_test = loader.get_labels(center_nodes)\n",
        "\n",
        "    num_nodes_batch = len(all_nodes)\n",
        "\n",
        "    print(f\"Test batch ready: {len(center_nodes)} central nodes, {num_nodes_batch} total\\n\")\n",
        "\n",
        "    # 4. Ultra memory-efficient Bayesian inference - ONE sample at a time\n",
        "    print(f\"Running Bayesian Monte Carlo inference ({n_mc} samples)...\")\n",
        "    print(\"Processing one sample at a time to minimize memory usage...\\n\")\n",
        "\n",
        "    # Initialize accumulators for online statistics (Welford's algorithm)\n",
        "    n_centers = len(center_nodes)\n",
        "    n_classes = y_test.shape[1]\n",
        "\n",
        "    # We'll accumulate mean and M2 for online variance calculation\n",
        "    mean_accumulator = jnp.zeros((n_centers, n_classes), dtype=jnp.float32)\n",
        "    m2_accumulator = jnp.zeros((n_centers, n_classes), dtype=jnp.float32)\n",
        "\n",
        "    # Pre-compute layer 1 aggregation (doesn't depend on weights)\n",
        "    mean_neighbor_layer1 = jraph.segment_mean(X_test[senders], receivers, num_nodes_batch)\n",
        "\n",
        "    # Process one sample at a time\n",
        "    for i in tqdm(range(n_mc), desc=\"MC Inference\", ncols=80):\n",
        "        # Extract i-th sample from posterior\n",
        "        w1 = posterior_samples['w1'][i]\n",
        "        b1 = posterior_samples['b1'][i]\n",
        "        w2 = posterior_samples['w2'][i]\n",
        "        b2 = posterior_samples['b2'][i]\n",
        "\n",
        "        # Forward pass - Layer 1\n",
        "        concat1 = jnp.concatenate([X_test, mean_neighbor_layer1], axis=-1)\n",
        "        h = jax.nn.relu(concat1 @ w1 + b1)\n",
        "\n",
        "        # Forward pass - Layer 2\n",
        "        mean_neighbor2 = jraph.segment_mean(h[senders], receivers, num_nodes_batch)\n",
        "        concat2 = jnp.concatenate([h, mean_neighbor2], axis=-1)\n",
        "        logits = concat2 @ w2 + b2\n",
        "        pred_all = jax.nn.sigmoid(logits)\n",
        "\n",
        "        # Extract predictions for center nodes only\n",
        "        pred = pred_all[center_indices]\n",
        "\n",
        "        # Update online statistics (Welford's algorithm)\n",
        "        delta = pred - mean_accumulator\n",
        "        mean_accumulator += delta / (i + 1)\n",
        "        delta2 = pred - mean_accumulator\n",
        "        m2_accumulator += delta * delta2\n",
        "\n",
        "        # Explicit cleanup every 10 samples\n",
        "        if (i + 1) % 10 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "    # Final statistics\n",
        "    mean_pred = mean_accumulator\n",
        "    variance = m2_accumulator / n_mc\n",
        "    uncertainty = jnp.sqrt(variance)\n",
        "\n",
        "    # Clear memory\n",
        "    del mean_accumulator, m2_accumulator, variance\n",
        "    gc.collect()\n",
        "\n",
        "    # 5. Results\n",
        "    acc = ((mean_pred > 0.5) == (y_test > 0.5)).mean() * 100\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BAYESIAN GRAPHSAGE — TEST SET RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Accuracy Test (Monte Carlo average): {acc:.2f}%\")\n",
        "    print(f\"Average epistemic uncertainty: {uncertainty.mean():.5f}\")\n",
        "    print(f\"Maximum uncertainty: {uncertainty.max():.5f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return mean_pred, uncertainty, y_test, acc\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HEqxPk5r1Ivt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Visualization function***"
      ],
      "metadata": {
        "id": "ZC5TarKN1Ivt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bayesian_prediction(mean_pred, uncertainty, y_test, acc, node_idx=0, num_classes=30):\n",
        "    \"\"\"\n",
        "    Visualize Bayesian prediction for a single node\n",
        "    \"\"\"\n",
        "    mean_node = mean_pred[node_idx]\n",
        "    unc_node = uncertainty[node_idx]\n",
        "    true_node = y_test[node_idx]\n",
        "\n",
        "    x = np.arange(num_classes)\n",
        "    fig, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "    bars = ax.bar(x, mean_node[:num_classes], yerr=unc_node[:num_classes],\n",
        "                  capsize=5, color='cornflowerblue', edgecolor='navy', alpha=0.8,\n",
        "                  label='Prediction ± epistemic uncertainty')\n",
        "\n",
        "    # True positive classes (red dots)\n",
        "    positive = np.where(true_node[:num_classes] > 0.5)[0]\n",
        "    ax.scatter(positive, true_node[positive], color='red', s=120, marker='o',\n",
        "               label='True positive class', zorder=10)\n",
        "\n",
        "    ax.set_title(f'Bayesian GraphSAGE — Prediction on a test node #{node_idx}\\n'\n",
        "                 f'Accuracy: {acc:.2f}% | Average uncertainty: {uncertainty.mean():.5f}',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel('Class', fontsize=12)\n",
        "    ax.set_ylabel('Probability', fontsize=12)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f'C{i}' for i in range(num_classes)], rotation=45, ha='right')\n",
        "    ax.legend(fontsize=12)\n",
        "    ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "    # Text with values above bars\n",
        "    for bar, val, err in zip(bars, mean_node[:num_classes], unc_node[:num_classes]):\n",
        "        if val > 0.05 or err > 0.01:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, val + err + 0.02,\n",
        "                    f'{val:.2f} ± {err:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "YC2teYtd1Ivt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***DIAGNOSTIC - Check what's actually saved***"
      ],
      "metadata": {
        "id": "kyxnoWRU1Ivt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diagnose_saved_model(path_model_dir):\n",
        "    \"\"\"\n",
        "    Diagnose what's in the saved checkpoints\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "    from orbax.checkpoint import PyTreeCheckpointer\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DIAGNOSTIC REPORT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    checkpointer = PyTreeCheckpointer()\n",
        "\n",
        "    # Check posterior samples\n",
        "    posterior_path = os.path.join(path_model_dir, \"posterior_samples\")\n",
        "\n",
        "    if os.path.exists(posterior_path):\n",
        "        print(f\"\\n✓ Found posterior_samples at: {posterior_path}\")\n",
        "\n",
        "        try:\n",
        "            posterior_state = checkpointer.restore(posterior_path)\n",
        "\n",
        "            print(\"\\nPosterior samples keys:\")\n",
        "            for key in posterior_state.keys():\n",
        "                if hasattr(posterior_state[key], 'shape'):\n",
        "                    print(f\"  - {key}: shape={posterior_state[key].shape}, dtype={posterior_state[key].dtype}\")\n",
        "                else:\n",
        "                    print(f\"  - {key}: {type(posterior_state[key])}\")\n",
        "\n",
        "            # Check if shapes are correct\n",
        "            if 'w1' in posterior_state:\n",
        "                w1_shape = posterior_state['w1'].shape\n",
        "                print(f\"\\n✓ w1 shape: {w1_shape}\")\n",
        "                print(f\"  Expected: (num_samples, 16, 256)\")\n",
        "\n",
        "            if 'w2' in posterior_state:\n",
        "                w2_shape = posterior_state['w2'].shape\n",
        "                print(f\"✓ w2 shape: {w2_shape}\")\n",
        "                print(f\"  Expected: (num_samples, 512, 112)\")\n",
        "\n",
        "            return posterior_state\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n✗ Error loading posterior samples: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"\\n✗ Posterior samples not found at: {posterior_path}\")\n",
        "        return None"
      ],
      "metadata": {
        "trusted": true,
        "id": "6NCCyQjB1Ivt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***MAIN FUNCTIONS – MAIN WITH TRAINING, VALIDATION, INFERENCE AND VISUALIZATION***"
      ],
      "metadata": {
        "id": "LhS2-4DF1Ivt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # === LOADING DATASET ===\n",
        "    base_dir_input = os.path.join(path_dataset, \"proteins_graph_dataset\")\n",
        "    edges_parquet_file = os.path.join(base_dir_input, \"proteins_edges.parquet\")\n",
        "    nodes_parquet_file = os.path.join(base_dir_input, \"proteins_nodes.parquet\")\n",
        "\n",
        "    path_model_dir = os.path.join('/content','drive','MyDrive','model_BGSAGE')\n",
        "    os.makedirs(path_model_dir, exist_ok=True)\n",
        "\n",
        "    loader = OGBNProteinsLoader(\n",
        "        edges_parquet=edges_parquet_file,\n",
        "        nodes_parquet=nodes_parquet_file\n",
        "    )\n",
        "\n",
        "    # MCMC (posterior training) - with better hyperparameters\n",
        "    posterior_samples = run_mcmc(loader, batch_size=32, num_samples=500, warmup=500)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    val_batch = loader.sample_batch(batch_size=64, split=\"valid\")\n",
        "\n",
        "    # Create node mapping for validation\n",
        "    all_nodes = set()\n",
        "    for center, neigh in val_batch:\n",
        "        all_nodes.add(center)\n",
        "        all_nodes.update(neigh)\n",
        "\n",
        "    all_nodes = sorted(list(all_nodes))\n",
        "    node_to_idx = {nid: i for i, nid in enumerate(all_nodes)}\n",
        "\n",
        "    senders, receivers = [], []\n",
        "    for center, neigh in val_batch:\n",
        "        center_idx = node_to_idx[center]\n",
        "        for n in neigh:\n",
        "            n_idx = node_to_idx[n]\n",
        "            senders += [center_idx, n_idx]\n",
        "            receivers += [n_idx, center_idx]\n",
        "\n",
        "    graph_data = (jnp.array(senders), jnp.array(receivers), len(all_nodes))\n",
        "    X_val = jnp.array(loader.get_features(all_nodes))\n",
        "\n",
        "    # Get predictions with more samples\n",
        "    mean_pred, uncertainty = predict_with_uncertainty(posterior_samples, graph_data, X_val, n_samples=500)\n",
        "\n",
        "    # Get labels only for center nodes\n",
        "    center_nodes = [c for c, _ in val_batch]\n",
        "    center_indices = jnp.array([node_to_idx[c] for c in center_nodes])\n",
        "    y_val = loader.get_labels(center_nodes)\n",
        "\n",
        "    # Extract predictions for center nodes only\n",
        "    mean_pred_centers = mean_pred[center_indices]\n",
        "    uncertainty_centers = uncertainty[center_indices]\n",
        "\n",
        "    acc = ((mean_pred_centers > 0.5) == (y_val > 0.5)).mean() * 100\n",
        "    print(f\"\\nMicro-Accuracy: {acc:.2f}%\")\n",
        "    print(f\"Average uncertainty: {uncertainty_centers.mean():.4f}\")\n",
        "    print(f\"Max uncertainty: {uncertainty_centers.max():.4f}\")\n",
        "    print(f\"Min uncertainty: {uncertainty_centers.min():.4f}\")\n",
        "\n",
        "    plot_predictions(mean_pred_centers, uncertainty_centers, y_val, num_classes=30)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ALL DONE! Graph displayed, no errors.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nStarting model export...\")\n",
        "\n",
        "    sample_batch = loader.sample_batch(batch_size=8, split=\"valid\")\n",
        "    s, r = [], []\n",
        "    for center, neigh in sample_batch:\n",
        "        for n in neigh:\n",
        "            s += [center, n]\n",
        "            r += [n, center]\n",
        "\n",
        "    sample_x = loader.get_features([c for c, _ in sample_batch])[:4]\n",
        "    sample_senders = jnp.array(s[:200])\n",
        "    sample_receivers = jnp.array(r[:200])\n",
        "\n",
        "    export_model = BayesianGraphSAGE()  # čistý model\n",
        "\n",
        "    export_model_all_formats(\n",
        "        model=export_model,\n",
        "        posterior_samples=posterior_samples,\n",
        "        output_dir=path_model_dir\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "09ZQP51w1Ivt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***INFERENCE ON A LEARNED BAYESIAN GRAPHSAGE MODEL***"
      ],
      "metadata": {
        "id": "0dBPNswa1Ivu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, run diagnostics to see what's in the saved files\n",
        "posterior_samples = diagnose_saved_model(path_model_dir)\n",
        "\n",
        "# INFERENCE - Start with very conservative settings\n",
        "mean_pred, uncertainty, y_test, acc = load_and_run_inference(\n",
        "    loader,\n",
        "    path_model_dir,\n",
        "    n_mc=20,        # Start with just 20 samples\n",
        "    batch_size=16   # Very small batch\n",
        ")\n",
        "\n",
        "# VISUALIZE\n",
        "plot_bayesian_prediction(mean_pred, uncertainty, y_test, acc, node_idx=0, num_classes=30)"
      ],
      "metadata": {
        "trusted": true,
        "id": "InYEuY3J1Ivu"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}