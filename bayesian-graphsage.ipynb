{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":287651293,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Bayesian GraphSAGE: Scalable protein function prediction on the ogbn-proteins dataset** \nImplementing multi-label classification with uncertainty estimation using JAX/NNX and PyArrow.\n\n## ***Dataset description: ogbn-proteins (Biological network)***\n\nOn your laptop, you are working with a dataset from the **Open Graph Benchmark (OGB)** series, which represents the cutting edge in testing graph algorithms.\n\n- **Povaha dat:** Reprezentuje vztahy mezi proteiny v různých biologických organismech.\n- **Structure:**\n    - **Nodes (132,534):** Individual proteins.\n    - **Edges (39,473,625):** Represent 8 types of interactions (e.g. homology, co-expression).\n- **Input Features:** Nodes initially have no features of their own. Your process uses **8-dimensional edge vectors** (link features) that are aggregated into nodes as initial features.\n- **Target:** Multi-label classification. You predict **112 binary labels** at once, corresponding to the presence of a protein in various biological functions.\n\n## ***Teorie: Bayesian GraphSAGE***\n\nYour model in your laptop is not just a standard neural network, it's an inductive and probabilistic system.\n\n### ***A. GraphSAGE (Inductive learning)***\n\nUnlike transductive models (like GCN) that require the entire graph in memory, GraphSAGE learns the **aggregation function**. This means that after training, your model can process proteins it never saw during training.\n\n**Key mechanism (SAmple and aggreGatE):** The model selects a fixed number of neighbors for each node in (Neighbor Sampling) and performs a state update:\n\n$$\nh_v^{(k)} = \\sigma \\left( W^k \\cdot \\text{CONTACT} \\left( h_v^{(k-1)}, \\text{AGG} \\left( \\{ h_u^{(k-1)} , \\forall u \\in N(v) \\}  \\right) \\right) \\right)\n$$\n\n- You are using **Mean Aggregator** (neighbor averaging) in your laptop.\n\n### ***B. Bayesian Uncertainty (MC Dropout)***\n\nThe reason the model is called \"Bayesian\" is the implementation of **Monte Carlo Dropout**.\n\n- **Theory:** Gal & Ghahramani proved that the application of Dropout during inference is mathematically equivalent to the approximation of Gaussian processes.\n- **Usage in a laptop:** Even when testing (inference), you leave Dropout on. If you run the model N times, you get a distribution of results. The mean is your prediction and the variance is your uncertainty.\n\n## ***Tutorial Description (Implementation process on a laptop)***\n\nThe notebook is divided into logical blocks that form the production pipeline:\n\n### ***Phase 1: Data Layer (Hybrid Storage)***\n\nHere you implement the critical solution for large graphs:\n\n- **PyArrow (Parquet):** Use it to store 39 million edges and node properties. Thanks to memory-mapping, you only load what you need.\n- **SQLite:** Serves as a fast topological index. The model asks SQLite: \"Who are the neighbors of node 42?\" and SQLite returns the rows in the Parquet file.\n\n### ***Phase 2: Model Architecture in NNX***\n\nYou are using **JAX NNX**, which is an object-oriented API.\n\n- **Neighbor Sampling:** At each training step, your dataloader randomly selects 10-25 neighbors for each layer. This prevents memory overflow (Memory Out of Memory).\n- **Multi-label Head:** The last layer has 112 outputs with **Sigmoid** activation.\n\n### ***Phase 3: Training and Optimization***\n\n- **Loss:** You use `Binary Cross Entropy` calculated over all 112 labels.\n- **JIT Kompilace:** Funkce `train_step` je obalena `@jax.jit`, což zkompiluje tvůj Python kód do vysoce optimalizovaného strojového kódu pro GPU/TPU (OpenXLA).\n\n## ***Why is this solution \"Production-Ready\"?***\n\nThis approach, which you have in your laptop, solves the three biggest pain points of graphing tasks:\n\n1. **Memory:** Thanks to PyArrow and neighbor sampling, you don't need 128GB of RAM.\n2. **Speed:** JAX and XLA compilations allow you to train millions of edges in minutes.\n3. **Credibility:** The Bayesian element gives your predictions a \"certificate of certainty\", which is essential in biochemistry or industry.\n\n***Summary of components for your documentation:***\n\n|Element|Implementation in a laptop|Theoretical benefit|\n|--------|-------------------------|-------------------|\n|**Data**|PyArrow + SQLite|Efficient I/O for giant graphs|\n|**Model**|GraphSAGE (NNX)|Inductive capability (new nodes)|\n|**Inference**|MC Dropout|Uncertainty estimation (Bayesian)|\n|**Engine**|JAX / OpenXLA|Maximum HW acceleration|","metadata":{}},{"cell_type":"markdown","source":"## ***Environment settings***","metadata":{}},{"cell_type":"code","source":"!rm -r /content/sample_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:26:28.273098Z","iopub.execute_input":"2025-12-29T15:26:28.273384Z","iopub.status.idle":"2025-12-29T15:26:28.398531Z","shell.execute_reply.started":"2025-12-29T15:26:28.273351Z","shell.execute_reply":"2025-12-29T15:26:28.397657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove existing JAX installations\n!pip uninstall -y -qq jax jaxlib jax-cuda12-plugin scipy pyarrow gymnasium numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:26:28.400302Z","iopub.execute_input":"2025-12-29T15:26:28.400571Z","iopub.status.idle":"2025-12-29T15:26:43.441604Z","shell.execute_reply.started":"2025-12-29T15:26:28.400543Z","shell.execute_reply":"2025-12-29T15:26:43.440711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install JAX \n!pip install -qq --upgrade \"jax[cpu12]\"\n!pip install scipy gymnasium==0.29.0\n!pip install tensorboard\n!pip install tensorboard-plugin-profile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:30:53.846223Z","iopub.execute_input":"2025-12-29T15:30:53.846972Z","execution_failed":"2025-12-29T15:31:34.155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install core dependencies\n%pip -qq install --upgrade jax jaxlib flax optax orbax-checkpoint grain\n%pip -qq install numpy matplotlib scipy\n%pip -qq install torch torchvision\n%pip -qq install datasets \n%pip -qq install msgpack requests tqdm\n%pip -qq install bitsandbytes\n%pip -qq install jraph\n%pip -qq install networkx\n%pip -qq install ogb\n%pip -qq install pyarrow\n%pip -qq install db-sqlite3\n%pip -qq install pandas polars\n%pip -qq install bitsandbytes numpyro langdetect\n%pip -qq install xprof\n%pip -qq install jax2onnx\n%pip -qq install onnx onnxruntime","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install Git LFS for large files\n!apt install git-lfs","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import IPython\nprint(\"Rebooting kernel... Please wait 5-10 seconds.\")\nIPython.Application.instance().kernel.do_shutdown(restart=True)\n!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -qq --upgrade \"jax[cuda12]\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(100*\"-\")\n%pip show jax\nprint(100*\"-\")\n%pip show jaxlib\nprint(100*\"-\")\n%pip show jax-cuda12-plugin\nprint(100*\"-\")\n%pip show flax\nprint(100*\"-\")\n%pip show optax\nprint(100*\"-\")\n%pip show torch\nprint(100*\"-\")\n%pip show torchvision\nprint(100*\"-\")\n%pip show orbax-checkpoint\nprint(100*\"-\")\n%pip show numpy\nprint(100*\"-\")\n%pip show tqdm\nprint(100*\"-\")\n%pip show datasets\nprint(100*\"-\")\n%pip show msgpack\nprint(100*\"-\")\n%pip show bitsandbytes\nprint(100*\"-\")\n%pip show jraph\nprint(100*\"-\")\n%pip show networkx\nprint(100*\"-\")\n%pip show ogb\nprint(100*\"-\")\n%pip show pyarrow\nprint(100*\"-\")\n%pip show db-sqlite3\nprint(100*\"-\")\n%pip show polars\nprint(100*\"-\")\n%pip show pandas\nprint(100*\"-\")\n%pip show grain\nprint(100*\"-\")\n%pip show bitsandbytes\nprint(100*\"-\")\n%pip show numpyro\nprint(100*\"-\")\n%pip show langdetect\nprint(100*\"-\")\n%pip show xprof\nprint(100*\"-\")\n%pip show jax2onnx\nprint(100*\"-\")\n%pip show onnx\nprint(100*\"-\")\n%pip show onnxruntime\nprint(100*\"-\")\n%pip show tensorboard-plugin-profile\nprint(100*\"-\")\n%pip show numpy\nprint(100*\"-\")\n%pip show matplotlib\nprint(100*\"-\")\n%pip show scipy\nprint(100*\"-\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Environment setup complete!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Import and configuration***","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport io\nimport glob\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport json\nimport time\nimport tensorflow as tf\nimport subprocess\nimport pickle\nimport zipfile\nimport base64\nimport shutil\nimport sqlite3\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom tqdm import tqdm\nimport networkx as nx\nfrom pathlib import Path\nimport functools\nfrom functools import partial\nfrom typing import (\n    Any,\n    Tuple,\n    Callable,\n    Optional,\n    Sequence,\n    List,\n    Dict\n)\nfrom IPython.display import clear_output\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nfrom ogb.nodeproppred import NodePropPredDataset\n\n# JAX and Flax NNX\nimport jax\nimport jax.ops\nimport jax.lax\nimport jax.profiler\nimport jax.numpy as jnp\nimport jax.export as jax_export\nfrom jax import (\n    random,\n    jit,\n    value_and_grad,\n    remat\n)\nimport jax.tree_util as tree_util\nimport flax.nnx as nnx\nfrom flax.nnx import filterlib\nfrom flax.serialization import (\n    msgpack_serialize,\n    from_bytes\n)\nimport orbax.checkpoint as ocp\nfrom orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n\n# Optimization\nimport optax\n\nimport jraph\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive\nfrom numpyro.contrib.module import nnx_module\n\nfrom datasets import load_dataset\n\nfrom kaggle_secrets import UserSecretsClient\n\n# PyTorch for compatibility (GGUF conversion)\nimport torch\n\nimport jax2onnx\nfrom jax2onnx import onnx_function, to_onnx\n\nimport onnx\nimport onnxruntime as ort\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Ensure JAX uses GPU if available***","metadata":{}},{"cell_type":"code","source":"# Configure JAX for GPU\ntry:\n    jax.config.update('jax_platform_name', 'gpu')\n    print(\"JAX devices:\", jax.devices())\nexcept RuntimeError:\n    print(\"GPU not available, using CPU\")\n    jax.config.update('jax_platform_name', 'cpu')\n    print(\"JAX devices:\", jax.devices())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Setting up access to Hugging Face***","metadata":{}},{"cell_type":"code","source":"def set_git_config(email, name):\n    try:\n        # Setting global user.email\n        subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", email], check=True)\n        print(f\"Git user.email set to: {email}\")\n\n        # Setting the global user.name\n        subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", name], check=True)\n        print(f\"Git user.name set to: {name}\")\n\n        # Check settings (optional)\n        email_output = subprocess.run([\"git\", \"config\", \"--global\", \"user.email\"], capture_output=True, text=True, check=True)\n        name_output = subprocess.run([\"git\", \"config\", \"--global\", \"user.name\"], capture_output=True, text=True, check=True)\n        print(f\"Check - Email: {email_output.stdout.strip()}\")\n        print(f\"Check - Name: {name_output.stdout.strip()}\")\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error while setting up Git configuration: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-29T15:28:27.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***SAGEConv layer***","metadata":{}},{"cell_type":"code","source":"class SAGEConv(nnx.Module):\n    def __init__(self, in_features: int, out_features: int, rngs=None):\n        self.linear = nnx.Linear(in_features * 2, out_features, rngs=rngs or nnx.Rngs(0))\n    \n    def __call__(self, x, senders, receivers):\n        mean_neighbor = jraph.segment_mean(x[senders], receivers, x.shape[0])\n        concatenated = jnp.concatenate([x, mean_neighbor], axis=-1)\n        return self.linear(concatenated)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Bayesian GraphSAGE model***","metadata":{}},{"cell_type":"code","source":"class BayesianGraphSAGE(nnx.Module):\n    def __init__(self, in_features=8, hidden_features=256, out_features=112):\n        rngs = nnx.Rngs(0)\n        self.sage1 = SAGEConv(in_features, hidden_features, rngs)\n        self.sage2 = SAGEConv(hidden_features, out_features, rngs)\n        self.dropout = nnx.Dropout(0.2, rngs=rngs)\n\n    def __call__(self, x, senders, receivers, training=False):\n        def core(x):\n            x = jax.nn.relu(self.sage1(x, senders, receivers))\n            x = self.sage2(x, senders, receivers)\n            return x\n        \n        x = remat(core)(x)\n        x = self.dropout(x, deterministic=not training)\n        return jax.nn.sigmoid(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Loader for OGBN-Proteins***","metadata":{}},{"cell_type":"code","source":"class OGBNProteinsLoader:\n    def __init__(self, edges_parquet, nodes_parquet):\n        self.edges = pq.read_table(edges_parquet).to_pandas()\n        self.nodes = pq.read_table(nodes_parquet).to_pandas()\n        \n        self.features = np.stack(self.nodes[\"features\"].values).astype(np.float32)\n        self.labels   = np.stack(self.nodes[\"labels\"].values).astype(np.float32)\n        \n        self.node_id_to_idx = {nid: i for i, nid in enumerate(self.nodes[\"node_id\"])}\n        \n        self.train_idx = self.nodes[self.nodes[\"split\"] == \"train\"][\"node_id\"].values\n        self.val_idx   = self.nodes[self.nodes[\"split\"] == \"valid\"][\"node_id\"].values\n        self.test_idx  = self.nodes[self.nodes[\"split\"] == \"test\"][\"node_id\"].values\n        \n        print(f\"Loaded: {len(self.nodes)} nodes, {len(self.edges)} edges\")\n\n    def get_neighbors(self, node_id, max_neighbors=25):\n        src = self.edges[self.edges[\"source\"] == node_id][\"target\"].values[:max_neighbors]\n        tgt = self.edges[self.edges[\"target\"] == node_id][\"source\"].values[:max_neighbors]\n        neigh = np.unique(np.concatenate([src, tgt]))\n        return neigh if len(neigh) > 0 else np.array([node_id])\n\n    def sample_batch(self, batch_size=64, max_neighbors=25, split=\"train\"):\n        idx = {\"train\": self.train_idx, \"valid\": self.val_idx, \"test\": self.test_idx}[split]\n        centers = np.random.choice(idx, batch_size, replace=False)\n        return [(c, self.get_neighbors(c, max_neighbors)) for c in centers]\n\n    def get_features(self, node_ids):\n        return self.features[[self.node_id_to_idx[nid] for nid in node_ids]]\n\n    def get_labels(self, node_ids):\n        return self.labels[[self.node_id_to_idx[nid] for nid in node_ids]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Bayesian model and inference***","metadata":{}},{"cell_type":"code","source":"def bayesian_graphsage_model(graph_data, X, y=None):\n    senders, receivers, num_nodes_batch = graph_data\n    \n    # Define priors for all model parameters with more reasonable scales\n    in_features, hidden_features, out_features = 8, 256, 112\n    \n    # sage1 parameters - use wider priors for better exploration\n    w1_scale = numpyro.sample('w1_scale', dist.HalfNormal(1.0))\n    w1 = numpyro.sample('w1', dist.Normal(0, w1_scale).expand([in_features * 2, hidden_features]).to_event(2))\n    b1 = numpyro.sample('b1', dist.Normal(0, 0.5).expand([hidden_features]).to_event(1))\n    \n    # sage2 parameters\n    w2_scale = numpyro.sample('w2_scale', dist.HalfNormal(1.0))\n    w2 = numpyro.sample('w2', dist.Normal(0, w2_scale).expand([hidden_features * 2, out_features]).to_event(2))\n    b2 = numpyro.sample('b2', dist.Normal(0, 0.5).expand([out_features]).to_event(1))\n    \n    # Forward pass\n    # Layer 1\n    mean_neighbor = jraph.segment_mean(X[senders], receivers, num_nodes_batch)\n    concat1 = jnp.concatenate([X, mean_neighbor], axis=-1)\n    h = jax.nn.relu(concat1 @ w1 + b1)\n    \n    # Layer 2\n    mean_neighbor2 = jraph.segment_mean(h[senders], receivers, num_nodes_batch)\n    concat2 = jnp.concatenate([h, mean_neighbor2], axis=-1)\n    logits = concat2 @ w2 + b2\n    preds = jax.nn.sigmoid(logits)\n    \n    # Add small epsilon to avoid numerical issues\n    preds = jnp.clip(preds, 1e-7, 1 - 1e-7)\n    \n    # Likelihood\n    with numpyro.plate(\"data\", X.shape[0]):\n        numpyro.sample(\"obs\", dist.Bernoulli(probs=preds).to_event(1), obs=y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_mcmc(loader, batch_size=32, num_samples=500, warmup=500):\n    print(\"Preparing the training batch...\")\n    batch = loader.sample_batch(batch_size=batch_size, split=\"train\")\n    \n    # Create node ID to local index mapping\n    all_nodes = set()\n    for center, neigh in batch:\n        all_nodes.add(center)\n        all_nodes.update(neigh)\n    \n    all_nodes = sorted(list(all_nodes))\n    node_to_idx = {nid: i for i, nid in enumerate(all_nodes)}\n    \n    # Build edges with local indices\n    senders, receivers = [], []\n    for center, neigh in batch:\n        center_idx = node_to_idx[center]\n        for n in neigh:\n            n_idx = node_to_idx[n]\n            senders += [center_idx, n_idx]\n            receivers += [n_idx, center_idx]\n    \n    senders = jnp.array(senders)\n    receivers = jnp.array(receivers)\n    num_nodes_batch = len(all_nodes)\n    \n    graph_data = (senders, receivers, num_nodes_batch)\n    \n    # Get features and labels for all nodes in the batch\n    X = jnp.array(loader.get_features(all_nodes))\n    # Only get labels for center nodes\n    center_nodes = [c for c, _ in batch]\n    center_indices = jnp.array([node_to_idx[c] for c in center_nodes])\n    \n    # Create full label array (only center nodes have labels)\n    y_full = jnp.zeros((num_nodes_batch, 112), dtype=jnp.float32)\n    y_centers = jnp.array(loader.get_labels(center_nodes))\n    y_full = y_full.at[center_indices].set(y_centers)\n    \n    key = random.PRNGKey(42)\n    \n    # Use more samples and longer warmup for better convergence\n    kernel = NUTS(bayesian_graphsage_model, max_tree_depth=6)\n    mcmc = MCMC(kernel, num_warmup=warmup, num_samples=num_samples, num_chains=1)\n    \n    print(f\"Running MCMC (warmup={warmup}, samples={num_samples})...\")\n    mcmc.run(key, graph_data, X, y=y_full)\n    \n    # Print diagnostics\n    mcmc.print_summary()\n    \n    print(\"MCMC completed!\")\n    \n    return mcmc.get_samples()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Prediction with uncertainty***","metadata":{}},{"cell_type":"code","source":"def predict_with_uncertainty(posterior_samples, graph_data, X, n_samples=100):\n    senders, receivers, num_nodes_batch = graph_data\n    \n    preds = []\n    for i in tqdm(range(n_samples), desc=\"MC prediction\"):\n        # Extract parameters for this sample\n        w1 = posterior_samples['w1'][i]\n        b1 = posterior_samples['b1'][i]\n        w2 = posterior_samples['w2'][i]\n        b2 = posterior_samples['b2'][i]\n        \n        # Forward pass with sampled parameters\n        # Layer 1\n        mean_neighbor = jraph.segment_mean(X[senders], receivers, num_nodes_batch)\n        concat1 = jnp.concatenate([X, mean_neighbor], axis=-1)\n        h = jax.nn.relu(concat1 @ w1 + b1)\n        \n        # Layer 2\n        mean_neighbor2 = jraph.segment_mean(h[senders], receivers, num_nodes_batch)\n        concat2 = jnp.concatenate([h, mean_neighbor2], axis=-1)\n        logits = concat2 @ w2 + b2\n        pred = jax.nn.sigmoid(logits)\n        \n        preds.append(pred)\n    \n    preds = jnp.stack(preds)\n    return preds.mean(0), preds.std(0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Visualization of inference results***","metadata":{}},{"cell_type":"code","source":"def plot_predictions(mean_pred, uncertainty, true_labels, num_classes=30):\n    mean_node = mean_pred[0]\n    unc_node = uncertainty[0]\n    true_node = true_labels[0]\n    \n    x = np.arange(num_classes)\n    classes = [f'C{i}' for i in range(num_classes)]\n    \n    fig, ax = plt.subplots(figsize=(14, 7))\n    ax.bar(x, mean_node[:num_classes], yerr=unc_node[:num_classes], capsize=5, color='skyblue', alpha=0.8, label='Pre-values ± uncertainty of values')\n    positive = np.where(true_node[:num_classes] > 0.5)[0]\n    ax.plot(positive, true_node[positive], 'ro', markersize=8, label='True positive')\n    \n    ax.set_ylabel('Probability')\n    ax.set_title('Bayesian GraphSAGE – Prediction with uncertainty (first node)')\n    ax.set_xticks(x)\n    ax.set_xticklabels(classes, rotation=45)\n    ax.legend()\n    ax.grid(True, axis='y')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Export do všech formátů***","metadata":{}},{"cell_type":"code","source":"def export_model_all_formats(model, posterior_samples, output_dir=\"models\"):\n    \"\"\"\n    Fixed export with synchronous checkpointer to avoid async traceback\n    \"\"\"\n    import time\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(f\"\\n{'='*70}\")\n    print(\"EXPORT MODELU (Synchronous PyTreeCheckpointer)\")\n    print(\"=\"*70 + \"\\n\")\n\n    # Use synchronous checkpointer - no async threads\n    checkpointer = PyTreeCheckpointer()\n\n    # 1. Save model structure - separate graphdef and state\n    graphdef, param_state, rng_state = nnx.split(model, nnx.Param, nnx.RngState)\n    \n    # Save graphdef separately using pickle (it's not a pytree)\n    model_dir = os.path.join(output_dir, \"bayesian_graphsage_nnx\")\n    os.makedirs(model_dir, exist_ok=True)\n    \n    graphdef_path = os.path.join(model_dir, \"graphdef.pkl\")\n    with open(graphdef_path, 'wb') as f:\n        pickle.dump(graphdef, f)\n    print(f\"✓ GraphDef uložen jako pickle: {graphdef_path}\")\n    \n    # Save param_state and rng_state with Orbax (these ARE pytrees)\n    state_items = {\n        \"param_state\": param_state,\n        \"rng_state\": rng_state\n    }\n    \n    state_path = os.path.join(model_dir, \"state\")\n    checkpointer.save(state_path, state_items)\n    print(f\"✓ Model state uložen do: {state_path}\")\n\n    # 2. Save posterior samples (stacked)\n    sample_keys = list(posterior_samples.keys())\n    num_samples = len(next(iter(posterior_samples.values())))\n\n    stacked_state = {}\n    for key in sample_keys:\n        stacked_state[key] = jnp.stack(posterior_samples[key])\n\n    posterior_state = nnx.State(stacked_state)\n\n    posterior_path = os.path.join(output_dir, \"posterior_samples\")\n    checkpointer.save(posterior_path, posterior_state)\n    print(f\"✓ Posterior samples uloženy ({num_samples} samples) do: {posterior_path}\")\n\n    # CRITICAL: Wait for Orbax to finalize (prevents async traceback)\n    print(\"\\nWaiting for the finalization of Orbax checkpoints...\")\n    time.sleep(2)  # Give Orbax time to finish async operations\n    \n    print(f\"\\n{'='*70}\")\n    print(\"ALL DONE! Checkpoints are fully finalized.\")\n    print(\"=\"*70)\n    print(\"   • bayesian_graphsage_nnx/\")\n    print(\"     ├── graphdef.pkl          — model structure\")\n    print(\"     └── state/                — model parameters\")\n    print(\"   • posterior_samples/        — MCMC samples\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Memory-optimized inference - processes one sample at a time***","metadata":{}},{"cell_type":"code","source":"def load_and_run_inference(loader, path_model_dir, n_mc=50, batch_size=16):\n    \"\"\"\n    Ultra memory-optimized inference - one sample at a time with immediate cleanup\n    \"\"\"\n    \n    print(\"=== LOADING THE MODEL AND POSTERIOR SAMPLES ===\\n\")\n\n    checkpointer = PyTreeCheckpointer()\n\n    # 1. Load model checkpoint (we don't actually need graphdef for inference)\n    model_dir = os.path.join(path_model_dir, \"bayesian_graphsage_nnx\")\n\n    if not os.path.exists(model_dir):\n        raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n\n    print(\"✓ Model directory found\")\n\n    # 2. Load posterior samples\n    posterior_path = os.path.join(path_model_dir, \"posterior_samples\")\n    \n    if not os.path.exists(posterior_path):\n        candidates = glob.glob(os.path.join(path_model_dir, \"posterior_samples*\"))\n        if not candidates:\n            raise FileNotFoundError(\"No directory with posterior samples found!\")\n        posterior_path = sorted(candidates)[-1]\n\n    posterior_state = checkpointer.restore(posterior_path)\n    posterior_samples = {k: posterior_state[k] for k in posterior_state.keys()}\n    num_samples_total = posterior_state[list(posterior_state.keys())[0]].shape[0]\n\n    print(f\"✓ Loaded {num_samples_total} posterior samples\\n\")\n\n    # 3. Prepare test batch with SMALLER size to reduce memory\n    print(\"I'm preparing a test batch...\")\n    test_batch = loader.sample_batch(batch_size=batch_size, split=\"test\")\n    \n    # Build node mapping for test batch\n    all_nodes = set()\n    for center, neigh in test_batch:\n        all_nodes.add(center)\n        all_nodes.update(neigh)\n    \n    all_nodes = sorted(list(all_nodes))\n    node_to_idx = {nid: i for i, nid in enumerate(all_nodes)}\n    \n    # Build edges with local indices\n    senders_list, receivers_list = [], []\n    for center, neigh in test_batch:\n        center_idx = node_to_idx[center]\n        for n in neigh:\n            n_idx = node_to_idx[n]\n            senders_list += [center_idx, n_idx]\n            receivers_list += [n_idx, center_idx]\n\n    senders = jnp.array(senders_list)\n    receivers = jnp.array(receivers_list)\n    \n    # Get features and labels for all nodes\n    X_test = jnp.array(loader.get_features(all_nodes))\n    \n    # Get labels only for center nodes\n    center_nodes = [c for c, _ in test_batch]\n    center_indices = jnp.array([node_to_idx[c] for c in center_nodes])\n    y_test = loader.get_labels(center_nodes)\n    \n    num_nodes_batch = len(all_nodes)\n\n    print(f\"Test batch ready: {len(center_nodes)} central nodes, {num_nodes_batch} total\\n\")\n\n    # 4. Ultra memory-efficient Bayesian inference - ONE sample at a time\n    print(f\"Running Bayesian Monte Carlo inference ({n_mc} samples)...\")\n    print(\"Processing one sample at a time to minimize memory usage...\\n\")\n    \n    # Initialize accumulators for online statistics (Welford's algorithm)\n    n_centers = len(center_nodes)\n    n_classes = y_test.shape[1]\n    \n    # We'll accumulate mean and M2 for online variance calculation\n    mean_accumulator = jnp.zeros((n_centers, n_classes), dtype=jnp.float32)\n    m2_accumulator = jnp.zeros((n_centers, n_classes), dtype=jnp.float32)\n    \n    # Pre-compute layer 1 aggregation (doesn't depend on weights)\n    mean_neighbor_layer1 = jraph.segment_mean(X_test[senders], receivers, num_nodes_batch)\n    \n    # Process one sample at a time\n    for i in tqdm(range(n_mc), desc=\"MC Inference\", ncols=80):\n        # Extract i-th sample from posterior\n        w1 = posterior_samples['w1'][i]\n        b1 = posterior_samples['b1'][i]\n        w2 = posterior_samples['w2'][i]\n        b2 = posterior_samples['b2'][i]\n        \n        # Forward pass - Layer 1\n        concat1 = jnp.concatenate([X_test, mean_neighbor_layer1], axis=-1)\n        h = jax.nn.relu(concat1 @ w1 + b1)\n        \n        # Forward pass - Layer 2\n        mean_neighbor2 = jraph.segment_mean(h[senders], receivers, num_nodes_batch)\n        concat2 = jnp.concatenate([h, mean_neighbor2], axis=-1)\n        logits = concat2 @ w2 + b2\n        pred_all = jax.nn.sigmoid(logits)\n        \n        # Extract predictions for center nodes only\n        pred = pred_all[center_indices]\n        \n        # Update online statistics (Welford's algorithm)\n        delta = pred - mean_accumulator\n        mean_accumulator += delta / (i + 1)\n        delta2 = pred - mean_accumulator\n        m2_accumulator += delta * delta2\n        \n        # Explicit cleanup every 10 samples\n        if (i + 1) % 10 == 0:\n            gc.collect()\n    \n    # Final statistics\n    mean_pred = mean_accumulator\n    variance = m2_accumulator / n_mc\n    uncertainty = jnp.sqrt(variance)\n    \n    # Clear memory\n    del mean_accumulator, m2_accumulator, variance\n    gc.collect()\n\n    # 5. Results\n    acc = ((mean_pred > 0.5) == (y_test > 0.5)).mean() * 100\n\n    print(f\"\\n{'='*60}\")\n    print(f\"BAYESIAN GRAPHSAGE — TEST SET RESULTS\")\n    print(f\"{'='*60}\")\n    print(f\"Accuracy Test (Monte Carlo average): {acc:.2f}%\")\n    print(f\"Average epistemic uncertainty: {uncertainty.mean():.5f}\")\n    print(f\"Maximum uncertainty: {uncertainty.max():.5f}\")\n    print(f\"{'='*60}\\n\")\n\n    return mean_pred, uncertainty, y_test, acc\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Visualization function***","metadata":{}},{"cell_type":"code","source":"def plot_bayesian_prediction(mean_pred, uncertainty, y_test, acc, node_idx=0, num_classes=30):\n    \"\"\"\n    Visualize Bayesian prediction for a single node\n    \"\"\"\n    mean_node = mean_pred[node_idx]\n    unc_node = uncertainty[node_idx]\n    true_node = y_test[node_idx]\n\n    x = np.arange(num_classes)\n    fig, ax = plt.subplots(figsize=(15, 8))\n\n    bars = ax.bar(x, mean_node[:num_classes], yerr=unc_node[:num_classes],\n                  capsize=5, color='cornflowerblue', edgecolor='navy', alpha=0.8,\n                  label='Prediction ± epistemic uncertainty')\n\n    # True positive classes (red dots)\n    positive = np.where(true_node[:num_classes] > 0.5)[0]\n    ax.scatter(positive, true_node[positive], color='red', s=120, marker='o',\n               label='True positive class', zorder=10)\n\n    ax.set_title(f'Bayesian GraphSAGE — Prediction on a test node #{node_idx}\\n'\n                 f'Accuracy: {acc:.2f}% | Average uncertainty: {uncertainty.mean():.5f}',\n                 fontsize=16, fontweight='bold')\n    ax.set_xlabel('Class', fontsize=12)\n    ax.set_ylabel('Probability', fontsize=12)\n    ax.set_xticks(x)\n    ax.set_xticklabels([f'C{i}' for i in range(num_classes)], rotation=45, ha='right')\n    ax.legend(fontsize=12)\n    ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n    ax.set_ylim(0, 1.05)\n\n    # Text with values above bars\n    for bar, val, err in zip(bars, mean_node[:num_classes], unc_node[:num_classes]):\n        if val > 0.05 or err > 0.01:\n            ax.text(bar.get_x() + bar.get_width()/2, val + err + 0.02,\n                    f'{val:.2f} ± {err:.2f}', ha='center', va='bottom', fontsize=9)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***DIAGNOSTIC - Check what's actually saved***","metadata":{}},{"cell_type":"code","source":"def diagnose_saved_model(path_model_dir):\n    \"\"\"\n    Diagnose what's in the saved checkpoints\n    \"\"\"\n    import os\n    import pickle\n    from orbax.checkpoint import PyTreeCheckpointer\n    \n    print(\"=\"*70)\n    print(\"DIAGNOSTIC REPORT\")\n    print(\"=\"*70)\n    \n    checkpointer = PyTreeCheckpointer()\n    \n    # Check posterior samples\n    posterior_path = os.path.join(path_model_dir, \"posterior_samples\")\n    \n    if os.path.exists(posterior_path):\n        print(f\"\\n✓ Found posterior_samples at: {posterior_path}\")\n        \n        try:\n            posterior_state = checkpointer.restore(posterior_path)\n            \n            print(\"\\nPosterior samples keys:\")\n            for key in posterior_state.keys():\n                if hasattr(posterior_state[key], 'shape'):\n                    print(f\"  - {key}: shape={posterior_state[key].shape}, dtype={posterior_state[key].dtype}\")\n                else:\n                    print(f\"  - {key}: {type(posterior_state[key])}\")\n            \n            # Check if shapes are correct\n            if 'w1' in posterior_state:\n                w1_shape = posterior_state['w1'].shape\n                print(f\"\\n✓ w1 shape: {w1_shape}\")\n                print(f\"  Expected: (num_samples, 16, 256)\")\n                \n            if 'w2' in posterior_state:\n                w2_shape = posterior_state['w2'].shape\n                print(f\"✓ w2 shape: {w2_shape}\")\n                print(f\"  Expected: (num_samples, 512, 112)\")\n                \n            return posterior_state\n            \n        except Exception as e:\n            print(f\"\\n✗ Error loading posterior samples: {e}\")\n            return None\n    else:\n        print(f\"\\n✗ Posterior samples not found at: {posterior_path}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***MAIN FUNCTIONS – MAIN WITH TRAINING, VALIDATION, INFERENCE AND VISUALIZATION***","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === LOADING DATASET ===\n    base_dir_input = os.path.join(\"/kaggle\",\"input\",\"bayesian-graphsage-dataset\",\"datasets\",\"graph_dataset\")\n    edges_parquet_file = os.path.join(base_dir_input, \"proteins_edges.parquet\")\n    nodes_parquet_file = os.path.join(base_dir_input, \"proteins_nodes.parquet\")\n\n    path_model_dir = os.path.join('/kaggle','working','models')\n    os.makedirs(path_model_dir, exist_ok=True)\n    \n    loader = OGBNProteinsLoader(\n        edges_parquet=edges_parquet_file,\n        nodes_parquet=nodes_parquet_file\n    )\n\n    # MCMC (posterior training) - with better hyperparameters\n    posterior_samples = run_mcmc(loader, batch_size=32, num_samples=500, warmup=500)\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"VALIDATION\")\n    print(\"=\"*70)\n    \n    val_batch = loader.sample_batch(batch_size=64, split=\"valid\")\n    \n    # Create node mapping for validation\n    all_nodes = set()\n    for center, neigh in val_batch:\n        all_nodes.add(center)\n        all_nodes.update(neigh)\n    \n    all_nodes = sorted(list(all_nodes))\n    node_to_idx = {nid: i for i, nid in enumerate(all_nodes)}\n    \n    senders, receivers = [], []\n    for center, neigh in val_batch:\n        center_idx = node_to_idx[center]\n        for n in neigh:\n            n_idx = node_to_idx[n]\n            senders += [center_idx, n_idx]\n            receivers += [n_idx, center_idx]\n    \n    graph_data = (jnp.array(senders), jnp.array(receivers), len(all_nodes))\n    X_val = jnp.array(loader.get_features(all_nodes))\n    \n    # Get predictions with more samples\n    mean_pred, uncertainty = predict_with_uncertainty(posterior_samples, graph_data, X_val, n_samples=500)\n    \n    # Get labels only for center nodes\n    center_nodes = [c for c, _ in val_batch]\n    center_indices = jnp.array([node_to_idx[c] for c in center_nodes])\n    y_val = loader.get_labels(center_nodes)\n    \n    # Extract predictions for center nodes only\n    mean_pred_centers = mean_pred[center_indices]\n    uncertainty_centers = uncertainty[center_indices]\n    \n    acc = ((mean_pred_centers > 0.5) == (y_val > 0.5)).mean() * 100\n    print(f\"\\nMicro-Accuracy: {acc:.2f}%\")\n    print(f\"Average uncertainty: {uncertainty_centers.mean():.4f}\")\n    print(f\"Max uncertainty: {uncertainty_centers.max():.4f}\")\n    print(f\"Min uncertainty: {uncertainty_centers.min():.4f}\")\n\n    plot_predictions(mean_pred_centers, uncertainty_centers, y_val, num_classes=30)\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"ALL DONE! Graph displayed, no errors.\")\n    print(\"=\"*70)\n\n    print(\"\\nStarting model export...\")\n    \n    sample_batch = loader.sample_batch(batch_size=8, split=\"valid\")\n    s, r = [], []\n    for center, neigh in sample_batch:\n        for n in neigh:\n            s += [center, n]\n            r += [n, center]\n    \n    sample_x = loader.get_features([c for c, _ in sample_batch])[:4]\n    sample_senders = jnp.array(s[:200])\n    sample_receivers = jnp.array(r[:200])\n    \n    export_model = BayesianGraphSAGE()  # čistý model\n\n    export_model_all_formats(\n        model=export_model,\n        posterior_samples=posterior_samples,\n        output_dir=path_model_dir\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***INFERENCE ON A LEARNED BAYESIAN GRAPHSAGE MODEL***","metadata":{}},{"cell_type":"code","source":"# First, run diagnostics to see what's in the saved files\nposterior_samples = diagnose_saved_model(path_model_dir)\n\n# INFERENCE - Start with very conservative settings\nmean_pred, uncertainty, y_test, acc = load_and_run_inference(\n    loader, \n    path_model_dir,\n    n_mc=20,        # Start with just 20 samples\n    batch_size=16   # Very small batch\n)\n    \n# VISUALIZE\nplot_bayesian_prediction(mean_pred, uncertainty, y_test, acc, node_idx=0, num_classes=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}